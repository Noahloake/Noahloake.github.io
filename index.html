<!--è¿™æ®µä»£ç æ˜¯ HTML æ–‡ä»¶çš„<head>éƒ¨åˆ†ã€‚å®ƒæœ¬èº«åœ¨ç½‘é¡µä¸Šä¸å¯è§ï¼Œä½†åŒ…å«äº†æ‰€æœ‰ç»™æµè§ˆå™¨å’Œæœç´¢å¼•æ“Žçœ‹çš„é‡è¦å…ƒä¿¡æ¯ï¼ˆmetadataï¼‰ï¼Œå†³å®šäº†ç½‘é¡µçš„åŸºæœ¬å±žæ€§å’Œè¡Œä¸ºã€‚-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="icon" href="./pic/PKUlogo.png">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="NoahLoake, Noah Loake, Noah Loake Peking University"> 
<meta name="description" content="Personal website of Noah Loake, an Economics PhD candidate at Peking University (PKU). Research interests include public economics, political economy, and Chinese local governance.">
<meta name="google-site-verification" content="7Z9sQkY0ra0BMMZRgx9fYr8E_6n4UuLd877xMuzhPVY" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Noah Loake@Guanghua, PKU</title>
<style>
    .smaller-image {
      width: 20%;
    }
</style>

<!--å®¹å™¨ç©ºé—´è®¾å®š-->
</head>
<body>
<div id="layout-content" style="margin-top:25px">
 
<!--ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸ªäººåŸºæœ¬ä¿¡æ¯æƒ…å†µè¯´æ˜Ž-->
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Noah Loake </h1>
					</div>
				<h3>ðŸŽ“ Seconde Year Phd Student</h3>
				<p> Department of Applied Economics </p>
				<p>Guanghua School of Management, Peking University. </p>
				<p><i>Email</i>: <a href="benchengwang@stu.pku.edu.cn">benchengwang@stu.pku.edu.cn</a></p>
				<p><i>Office</i>: Peking University Science Park, 305-153, No.5 Yiheyuan Road, Haidian District.</p>
				<p><p>I collect and organize information about economics seminars at Peking University and Tsinghua University each semester. If you're interested, you can refer to this shared spreadsheet: <a href="https://docs.qq.com/sheet/DUVZZektqV2ViS255?tab=BB08J2" target="_blank"><strong>[Tencent Doc] Econ Seminar</strong></a></p></p>
			</td>
			<td>
				<img src="./pic/Bencheng.jpg" border="0" width="200"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<h2>Biography</h2>
<p>
	Hello! I am a first-year Phd student of CSE at <a href="https://hkust.edu.hk/">The Hong Kong University of Science and Technology (HKUST)</a>, under the supervision of <a href="https://cqf.io/" target="_blank">Prof. Qifeng Chen</a>. 
	I obtained my M.S. in Computer Science at <a href="https://www.tsinghua.edu.cn/en/index.htm">Tsinghua University</a> in 2024, supervised by Prof. Xiu Li 
	and B.Eng in Computer Science at <a href="https://www.tyut.edu.cn/">Taiyuan University of Technology</a> in 2024. 
</p>
<p>	
	I am currently a Research Scientist Intern at Meta. Previously, I studied in the <a href="http://mmlab.siat.ac.cn/">MMLab@SIAT</a> led by Prof. <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en" target="_blank">Dr. Yu Qiao</a></a>.
	I was also fortunate to be an internship at Baidu, Tencent AI Lab and <a href="https://hunyuan.tencent.com//">Tencent hunyuan</a>.
</p>

<p>My research interests lie in the intersection of <b>Computer Vision</b> and  <b>Machine Learning</b>. From 2021, I started to do some research on video understanding and self-supervised learning. Now, 
	I focus on designing novel applications for image/video generation(ðŸ‘ª Follow Family), world model, and other downstream AIGC tasks.</p>

<p><i style="color: red; display: inline;">Feel free to contact me by email if you are interested in discussing or collaborating with me.</i></p>


<h2>News</h2>
<div style="height: 200px; overflow: auto;">
<ul>
	<li>
		[04/2025] One paper are accepted by <a href="https://aaai.org/conference/aaai/aaai-25/">ACM MM 2025</a> !  
	</li>
</ul>
</div>

	


<h2>Education & Visiting</h2>
<table id="tbPublications" width="100%">
	<tbody>			
		<td>
		<p><b>The Hong Kong University of Science and Technology, Hong Kong</b></p>
		<p>PhD Student in Visual Intelligence Lab, HKUST </p>
		<p>Advisor: <a href="https://cqf.io/">Prof. Qifeng Chen</a> </p>
		<p>Sep. 2024 - Future <p>
		</p>
		</td>
	</tr>
	<!--########################-->
	<tbody>			
		<td>
		<p><b>The Hong Kong University of Science and Technology, Hong Kong</b></p>
		<p>Research Assistant in Visual Intelligence Lab, HKUST </p>
		<p>Advisor: <a href="https://cqf.io/">Prof. Qifeng Chen</a> </p>
		<p>July. 2023 - Nov. 2023 <p>
		</p>
		</td>
	</tr>
	<!--########################-->
	<p></p>
	</td>
	<tr>
	<!-- <tr>&nbsp</tr>
	<tr>&nbsp</tr> -->
	<tr>
			
		<td>
		<p><b>Tsinghua University, China</b></p>
		<p>Master of Engineering in Electronic Information </p>
		<p>Advisor: <a href="https://scholar.google.com/citations?user=Xrh1OIUAAAAJ&hl=zh-CN">Prof.Xiu Li</a></p>
		<p>Sep. 2021 - Jun. 2024 <p>
		</p>
		</td>
	</tr>

	<!--########################-->
		<p></p>
	</td>
	<tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>				
		<td>
		<p><b> University of Chinese Academy of Sciences, China</b></p>
		<p>Research Assistant in Multimedia Laboratory, SIAT, CAS (<a href="https://mmlab.siat.ac.cn/">MMLab@SIAT</a>) </p>
		<p>Advisor: <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=gFtI-8QAAAAJ&view_op=list_works&sortby=pubdate">Prof. Yu Qiao</a> and  <a href="https://scholar.google.com.hk/citations?user=hD948dkAAAAJ&hl=zh-CN">Dr. Yali Wang</a></p>
		<p>Jul. 2021 - Apr. 2022 <p>
		</p>
		</td>
	</tr>
	<!--########################-->
	<tr>
		</tr>			
			<td>
			<p><b>Taiyuan University of technology, China</b></p>
			<p>Bachelor of Engineering in Computer Science </p>
			<p>Sep. 2017 - Jun. 2021 <p>
			</p>
			</td>
		</tr>
		<tr>&nbsp</tr>

	<!--########################-->
		
</tbody></table>


<h2> Selected Publications | <a href="https://scholar.google.com/citations?user=kwBR1ygAAAAJ&hl=zh-CN">Full List</a></h2>
<!--
<div style="height: 1440px; overflow: auto;">
-->
<table id="tbPublications" width="100%">
	<tbody>
	<td><b>/*Preprints*/</b>
	<p></p>
	</td>
	<tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2025-survey.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>ðŸŽ‰ Controllable Video Generation: A Survey</b></p>
		<p><b>Yue Ma</b>, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, Zeyu Wang, Zhifeng Li, Xiu Li, Wei Liu, Dan Xu, Linfeng Zhang, Qifeng Chen</p>
		<em>arXiv preprint:2507.16869. 2025</em>
		<p> [<a href="https://arxiv.org/pdf/2507.16869">paper</a>] [<a href="https://github.com/mayuelala/Awesome-Controllable-Video-Generation">code</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2025-nips.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>ðŸŽ‰ Follow-Your-Creation: Empowering 4D Creation through Video Inpainting</b></p>
		<p><b>Yue Ma</b>, Kunyu Feng, Xinhua Zhang,   Hongyu Liu, David Junhao Zhang,   Jinbo Xing,   Yinhan Zhang,   Ayden Yang,  Zeyu Wang,  Qifeng Chen</p>
		<em>arXiv preprint:2506.04590. 2025</em>
		<p> [<a href="https://arxiv.org/pdf/2506.04590">paper</a>] [<a href="https://follow-your-creation.github.io/">code</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2025-motion.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>ðŸŽ‰ Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning</b></p>
		<p><b>Yue Ma</b>, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, Qifeng Chen</p>
		<em>arXiv preprint:2506.05207. 2025</em>
		<p> [<a href="https://arxiv.org/pdf/2506.05207">paper</a>] [<a href="https://follow-your-motion.github.io/">code</a>] </p>
		</td>
	</tr>
	<!--########################-->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2022-simvtp-framework.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>SimVTP: Simple Video Text Pre-training with Masked Autoencoders</b></p>
		<p><b>Yue Ma</b>, Tianyu Yang, Ying Shan, Xiu Li</p>
		<em>arXiv preprint:2211.03490. 2022</em>
		<p> [<a href="https://arxiv.org/pdf/2211.03490">paper</a>] [<a href="https://github.com/mayuelala/SimVTP">code</a>] </p>
		</td>
	</tr> -->
	<!--########################-->

	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<td><b>/*Conference*/</b>
	<p></p>
	</td>
	<tr>	
	<td width="306">
	<img src="./indexpics/2024-followyourclick.png" width="285px" style="box-shadow: 4px 4px 8px #888">
	</td>				
	<td>
	<p><b>Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts</b></p>
	<p><b>Yue Ma</b>, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, Heung-Yeung Shum, Wei Liu, Qifeng Chen</p>
	<em>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025</em>
	<p> [<a href="https://arxiv.org/abs/2403.08268">paper</a>] [<a href="https://github.com/mayuelala/FollowYourClick">code</a>] [<a href="https://follow-your-click.github.io/">project page</a>] 
	</p>
	</td>
	</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2024-Canvas.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation</b></p>
		<p>Qihua Chen*, <b>Yue Ma*</b>, Hongfa Wang*, Junkun Yuan*, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen, Wei Liu</p>
		<em>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025</em>
		<p> [<a href="hhttps://arxiv.org/abs/2409.01055">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://github.com/mayuelala">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>

		
	<!--########################-->
	<tr>
		<td width="306">
		<img src="./indexpics/2024-Fypv2.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation for Stable Pose Control</b></p>
		<p>Jingyun Xue, Hongfa Wang, Qi Tian, <b>Yue Ma</b>, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao Zhang, Heung-Yeung Shum, Wei Liu, Mengyang Liu, Wenhan Luo</p>
		<em>International Conference on Learning Representations(<b>ICLR</b>) 2025</em>
		<p> [<a href="hhttps://arxiv.org/abs/2406.03035">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://github.com/mayuelala">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<!--########################-->
	<tr></tr>
		<td width="306">
		<img src="./indexpics/2024-followyouremoji.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</b></p>
		<p><b>Yue Ma</b>, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, Qifeng Chen</p>
		<em>The ACM Special Interest Group for Computer Graphics and Interactive Techniques(<b>Siggraph Asia</b>) 2024</em>
		<p> [<a href="https://arxiv.org/abs/2406.01900">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://follow-your-emoji.github.io/">project page</a>] 
		</p>
		</td>
	</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2023-MagicStick.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Handle: Controllable Video Editing via Control Handle Transformations</b></p>
		<p><b>Yue Ma</b>, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, Qifeng Chen</p>
		<em>IEEE /CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2025</em>
		<p> [<a href="https://arxiv.org/abs/2312.03047">paper</a>] [<a href="https://github.com/mayuelala/MagicStick">code</a>] [<a href="https://magic-stick-edit.github.io/">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<!--########################-->
	<tr>
		<td width="306">
		<img src="./indexpics/2023-iccv-followyourpose.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>ðŸ•ºðŸ•ºðŸ•º Follow-Your-Pose ðŸ’ƒðŸ’ƒðŸ’ƒ: Pose-Guided Text-to-Video Generation using Pose-Free Videos</b></p>
		<p><b>Yue Ma</b>, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Ying Shan, Xiu Li, Qifeng Chen</p>
		<em>The 38th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024</em>
		<p><a href="https://www.paperdigest.org/2024/09/most-influential-aaai-papers-2024-09/" style="color: red; display: inline;"><b>PaperDigest Most Influential Papers of AAAI 24</b></a></p>
		<p> [<a href="https://arxiv.org/abs/2304.01186">paper</a>] [<a href="https://github.com/mayuelala/FollowYourPose">code</a>] [<a href="https://follow-your-pose.github.io/">project page</a>] 
		<a target="_blank" href ="https://github.com/mayuelala/FollowYourPose"><img alt="GitHub stars" align="right" src="https://img.shields.io/github/stars/mayuelala/FollowYourPose?style=social"></a><a></a></p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<!--########################-->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2024-AAAI-MBEV.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>M-BEV: Masked BEV Perception for Robust Autonomous Driving</b></p>
		<p>Siran Chen, <b>Yue Ma</b>, Yu Qiao, Yali Wang</p>
		<em>The 38th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024</em>
		<p> [<a href="https://arxiv.org/abs/2304.01186">paper</a>] [<a href="https://github.com/mayuelala/FollowYourPose">code</a>] [<a href="https://follow-your-pose.github.io/">project page</a>] 
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr> -->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2023-icassp-audio.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>SemanticAC: Semantics-Assisted Framework for Audio Classification</b></p>
		<p>Yicheng Xiao*, <b>Yue Ma*</b>, Shuyan Li, Hantao Zhou, Ran Liao, Xiu Li (* equal contribution)</p>
		<em>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>), 2023. </em>
		<i></i>
		<p> [<a href="https://arxiv.org/abs/2302.05940">paper</a>] [<a href="https://github.com/mayuelala">code</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr> -->
	<tr>
		<td width="306">
		<img src="./indexpics/2022-mm-graph.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Visual Knowledge Graph for Human Action Reasoning in Videos</b></p>
		<p><b>Yue Ma</b>, Yali Wang, Yue Wu, Ziyu Lyu, Siran Chen, Xiu Li, Yu Qiao</p>
		<em>The 30th ACM International Conference on Multimedia. (<b>ACM MM</b>), 2022. </em>
		<i><p style="color: red; display: inline;">(<b>Oral Presentation</b>)</p></i>
		<p> [<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548257">paper</a>] [<a href="https://github.com/mayuelala/AKU">code</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    	<tr>&nbsp</tr>
    	<tr>&nbsp</tr>
		
	<!-- <td><b>/*Journal*/</b>
		<p></p>
		</td>
		
		<tr>
			<td width="306">
			<img src="./indexpics/2023-TMM-mmlab.png" width="285px" style="box-shadow: 4px 4px 8px #888">
			</td>				
			<td>
			<p><b>Attentive Snippet Prompting for Video Retrieval</b></p>
			<p>Siran Chen, Qinglin Xu, <b>Yue Ma</b>, Yu Qiao, Yali Wang</p>
			<em>IEEE Transactions on Multimedia (<b>TMM</b>), 2024. </em>
			<i></i>
			<p> [<a href="https://ieeexplore.ieee.org/abstract/document/10268993/">paper</a>] [<a href="https://ieeexplore.ieee.org/abstract/document/10268993/">code</a>] </p>
			</td>
		</tr> -->
		<tr>&nbsp</tr>
			<tr>&nbsp</tr>
			<tr>&nbsp</tr>
			<tr>&nbsp</tr>
			<tr>&nbsp</tr>
	

</tbody></table>
<!--
</div>
-->




<h2>Honors &amp; Awards</h2>
<table style="border-spacing:2px">
	<tbody>
	<tr><td> [06/2024] Outstanding graduates student of Beijing.</td></tr>
	<tr><td> [08/2023] First-Class Scholarship of <a herf="https://www.tsinghua.edu.cn/">Tsinghua University</a>.</td></tr>
	<tr><td> [12/2022] First-Class Scholarship of <a herf="https://www.tsinghua.edu.cn/">SIGS</a>, Tsinghua University.</td></tr>
	<tr><td> [03/2022] <a href="https://www.withzz.com/project/detail/99">Tencent Rhino-Bird Research Elite Program</a>, only 72 students in the world admitted to this program.</td></tr>
	<tr><td> [09/2020] Scholarship for Academic Excellence of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>.
	<tr><td> [06/2019] Excellent Scientific Student of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>.
	<tr><td> [09/2019] Scholarship for Academic Excellence of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>
	<tr><td> [09/2018] Excellent Academic Progress Student of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>.
	<tr><td> [06/2018] Scholarship for Academic Excellence of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>.

	</tbody>
</table>


<h2>Teaching</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2025 Fall, 2024 Fall</td><td>Economics 101</td><td>(PKU, 02831110)</td>
		</tr>
	</tbody>
</table>




<div id="footer">
	<div id="footer-text"></div>
</div>
	<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=777777&w=487&t=tt&d=xeJu_Kwek6AfO5eDCKFQ1iDWjzFQPLT_dNcYY3WLmrY&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=5b1717'></script>
	<p><center> &copy;Noah Loake </center></p>


</div>
</body></html>
















