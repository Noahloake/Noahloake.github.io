<!--è¿™æ®µä»£ç æ˜¯ HTML æ–‡ä»¶çš„<head>éƒ¨åˆ†ã€‚å®ƒæœ¬èº«åœ¨ç½‘é¡µä¸Šä¸å¯è§ï¼Œä½†åŒ…å«äº†æ‰€æœ‰ç»™æµè§ˆå™¨å’Œæœç´¢å¼•æ“çœ‹çš„é‡è¦å…ƒä¿¡æ¯ï¼ˆmetadataï¼‰ï¼Œå†³å®šäº†ç½‘é¡µçš„åŸºæœ¬å±æ€§å’Œè¡Œä¸ºã€‚-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="icon" href="./pic/PKUlogo.png">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="NoahLoake, Noah Loake, Noah Loake Peking University"> 
<meta name="description" content="Personal website of Noah Loake, an Economics PhD candidate at Peking University (PKU). Research interests include public economics, political economy, and Chinese local governance.">
<meta name="google-site-verification" content="7Z9sQkY0ra0BMMZRgx9fYr8E_6n4UuLd877xMuzhPVY" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Noah Loake@Guanghua, PKU</title>
<style>
    .smaller-image {
      width: 20%;
    }
	/* CSS for Education Section Layout */
.education-entry {
    margin-bottom: 1.5em; /* ä¸ºæ¯ä¸ªæ•™è‚²ç»å†æ¡ç›®ä¹‹é—´å¢åŠ ä¸€äº›å‚ç›´é—´è· */
}
.entry-header {
    display: flex;
    justify-content: space-between;
    align-items: baseline;
}
.entry-header h4 {
    margin: 0; /* ç§»é™¤å¤§å­¦åç§°çš„é»˜è®¤è¾¹è·ï¼Œä½¿å…¶æ›´ç´§å‡‘ */
}
.education-entry p {
    margin: 0.25em 0; /* è°ƒæ•´æ®µè½çš„å‚ç›´è¾¹è·ï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´åè°ƒ */
}
	/* CSS for News Section */
.news-box {
    height: 100px; /* é«˜åº¦å‡åŠä¸º100px */
    overflow-y: auto; /* ä»…åœ¨å‚ç›´æ–¹å‘å†…å®¹æº¢å‡ºæ—¶æ˜¾ç¤ºæ»šåŠ¨æ¡ */
    border: 1px solid #e0e0e0; /* æ·»åŠ ä¸€ä¸ªæµ…ç°è‰²è¾¹æ¡†ï¼Œä½¿å…¶çœ‹èµ·æ¥åƒä¸ªç›’å­ */
    padding: 10px 15px; /* å¢åŠ å†…è¾¹è·ï¼Œè®©å†…å®¹ä¸è´´è¾¹ */
    border-radius: 5px; /* è½»å¾®çš„åœ†è§’ï¼Œæ›´å…·ç°ä»£æ„Ÿ */
    background-color: #f9f9f9; /* æ·¡æ·¡çš„èƒŒæ™¯è‰²ï¼Œä¸é¡µé¢å…¶ä»–éƒ¨åˆ†åŒºåˆ† */
}

.news-list {
    list-style-type: none; /* ç§»é™¤åˆ—è¡¨é»˜è®¤çš„é»‘ç‚¹ */
    padding-left: 0;
    margin: 0;
}

.news-list li {
    margin-bottom: 12px; /* å¢åŠ æ¯æ¡æ–°é—»ä¹‹é—´çš„é—´è· */
    line-height: 1.5; /* å¢åŠ è¡Œé«˜ï¼Œæå‡å¯è¯»æ€§ */
}

.news-date {
    display: inline-block; /* è®©æ—¥æœŸå¯ä»¥è®¾ç½®å†…å¤–è¾¹è· */
    background-color: #e7e7e7; /* æ—¥æœŸçš„èƒŒæ™¯è‰² */
    color: #333;
    padding: 2px 8px; /* æ—¥æœŸçš„å†…è¾¹è· */
    border-radius: 10px; /* èƒ¶å›Šå½¢çŠ¶çš„åœ†è§’ */
    font-size: 0.8em; /* å­—ä½“ç¨å° */
    font-weight: bold;
    margin-right: 10px; /* ä¸æ–°é—»å†…å®¹æ‹‰å¼€è·ç¦» */
}
</style>

<!--å®¹å™¨ç©ºé—´è®¾å®š-->
</head>
<body>
<div id="layout-content" style="margin-top:25px">
 
<!--ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸ªäººåŸºæœ¬ä¿¡æ¯æƒ…å†µè¯´æ˜-->
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Noah Loake </h1>
					</div>
				<h3>ğŸ“ Second Year Phd Student</h3>
				<p> Department of Applied Economics </p>
				<p>Guanghua School of Management, Peking University. </p>
				<p><i>Email</i>: <a href="benchengwang@stu.pku.edu.cn">benchengwang@stu.pku.edu.cn</a></p>
				<p><i>Office</i>: Peking University Science Park, 305-153, No.5 Yiheyuan Road, Haidian District.</p>
				<p>Notice:I collect and organize information about economics seminars at Peking University and Tsinghua University each semester. If you're interested, you can refer to this shared spreadsheet: <a href="https://docs.qq.com/sheet/DUVZZektqV2ViS255?tab=BB08J2" target="_blank"><strong>[Tencent Doc Link] Econ Seminar</strong></a>.</p>
			</td>
			<td>
				<img src="./pic/Bencheng.jpg" border="0" width="200"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<!--ç¬¬äºŒéƒ¨åˆ†ï¼šä¸ªäººç®€ä»‹-->
<h2>Biography</h2>
<p>
    I am a second-year PhD candidate in the Department of Applied Economics at the <a href="https://en.gsm.pku.edu.cn/index.htm" target="_blank">Guanghua School of Management, Peking University</a>. 
    I am fortunate to be advised by <a href="https://en.gsm.pku.edu.cn/faculty/x.zhang/" target="_blank">Prof. Xiaobo Zhang</a> and <a href="https://en.gsm.pku.edu.cn/faculty/zhoula/" target="_blank">Prof. Li-An Zhou</a>.
    Previously, I received my M.A. in Economics from <a href="http://en.ruc.edu.cn/" target="_blank">Renmin University of China</a> and a B.A. with a minor in Economics from <a href="https://en.whu.edu.cn/" target="_blank">Wuhan University</a>.
</p>
<p>
    My research interests include <b>Development Economics</b>, <b>Political Economics</b>, and <b>Firm Behavior</b>. 
    Specifically, my work focuses on topics such as <i>local government debt and payment arrears, rural-urban migration, as well as grassroots governance and social stability</i>. 
    I utilize large-scale firm-level and government-related datasets to explore government-business interactions within the context of China.
</p>

<p><i style="color: red; display: inline;">Feel free to contact me by email if you are interested in discussing or collaborating with me.</i></p>

<!--æ•™è‚²èƒŒæ™¯-->
<h3>Education</h3>

<div class="education-entry">
    <div class="entry-header">
        <h4>Peking University, Guanghua School of Management</h4>
        <span>Sep 2023 - Present</span>
    </div>
    <p>PhD Candidate in Applied Economics</p>
    <p><i>Dissertation Committee: <a href="https://en.gsm.pku.edu.cn/faculty/x.zhang/" target="_blank">Prof. Xiaobo Zhang</a>, <a href="https://en.gsm.pku.edu.cn/faculty/zhoula/" target="_blank">Prof. Li-An Zhou</a></i></p>
</div>

<div class="education-entry">
    <div class="entry-header">
        <h4>Renmin University of China, School of Economics</h4>
        <span>Sep 2021 - Jul 2023</span>
    </div>
    <p>M.A. in Economics</p>
</div>

<div class="education-entry">
    <div class="entry-header">
        <h4>Wuhan University, School of Economics and Management</h4>
        <span>Sep 2016 - Jul 2020</span>
    </div>
    <p>B.A. with a Minor in Economics</p>
</div>

	
<!--ç¬¬äºŒéƒ¨åˆ†ï¼šæˆ‘çš„æœ€æ–°æ¶ˆæ¯-->
	<h2>News</h2>
<div class="news-box">
    <ul class="news-list">
        <li>
            <span class="news-date">08/2025</span>
            ğŸ‰ My first English-language paper has been accepted for publication in <i><a href="https://www.sciencedirect.com/journal/finance-research-letters">Financial Research Letters</a></i> (FRL). 
        </li>
        <li>
            <span class="news-date">07/2025</span>
            Participated in fieldwork for the <a href="https://cer.gsm.pku.edu.cn/index.htm" target="_blank">Enterprise Survey on Innovation and Entrepreneurship in China (ESIEC)</a> across Zhejiang, Gansu, and Liaoning provinces. 
        </li>
        <li>
            <span class="news-date">05/2025</span>
            I presented my paper, "Connection-Based Favoritism Within the Courtroom," at the Development and Political Economy Seminar at PKU. 
        </li>
    </ul>
</div>
	


<h2> Selected Publications | <a href="https://scholar.google.com/citations?user=kwBR1ygAAAAJ&hl=zh-CN">Full List</a></h2>
<!--
<div style="height: 1440px; overflow: auto;">
-->
<table id="tbPublications" width="100%">
	<tbody>
	<td><b>/*Preprints*/</b>
	<p></p>
	</td>
	<tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2025-survey.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>ğŸ‰ Controllable Video Generation: A Survey</b></p>
		<p><b>Yue Ma</b>, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, Zeyu Wang, Zhifeng Li, Xiu Li, Wei Liu, Dan Xu, Linfeng Zhang, Qifeng Chen</p>
		<em>arXiv preprint:2507.16869. 2025</em>
		<p> [<a href="https://arxiv.org/pdf/2507.16869">paper</a>] [<a href="https://github.com/mayuelala/Awesome-Controllable-Video-Generation">code</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2025-nips.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>ğŸ‰ Follow-Your-Creation: Empowering 4D Creation through Video Inpainting</b></p>
		<p><b>Yue Ma</b>, Kunyu Feng, Xinhua Zhang,   Hongyu Liu, David Junhao Zhang,   Jinbo Xing,   Yinhan Zhang,   Ayden Yang,  Zeyu Wang,  Qifeng Chen</p>
		<em>arXiv preprint:2506.04590. 2025</em>
		<p> [<a href="https://arxiv.org/pdf/2506.04590">paper</a>] [<a href="https://follow-your-creation.github.io/">code</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2025-motion.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>ğŸ‰ Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning</b></p>
		<p><b>Yue Ma</b>, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, Qifeng Chen</p>
		<em>arXiv preprint:2506.05207. 2025</em>
		<p> [<a href="https://arxiv.org/pdf/2506.05207">paper</a>] [<a href="https://follow-your-motion.github.io/">code</a>] </p>
		</td>
	</tr>
	<!--########################-->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2022-simvtp-framework.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>SimVTP: Simple Video Text Pre-training with Masked Autoencoders</b></p>
		<p><b>Yue Ma</b>, Tianyu Yang, Ying Shan, Xiu Li</p>
		<em>arXiv preprint:2211.03490. 2022</em>
		<p> [<a href="https://arxiv.org/pdf/2211.03490">paper</a>] [<a href="https://github.com/mayuelala/SimVTP">code</a>] </p>
		</td>
	</tr> -->
	<!--########################-->

	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<td><b>/*Conference*/</b>
	<p></p>
	</td>
	<tr>	
	<td width="306">
	<img src="./indexpics/2024-followyourclick.png" width="285px" style="box-shadow: 4px 4px 8px #888">
	</td>				
	<td>
	<p><b>Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts</b></p>
	<p><b>Yue Ma</b>, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, Heung-Yeung Shum, Wei Liu, Qifeng Chen</p>
	<em>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025</em>
	<p> [<a href="https://arxiv.org/abs/2403.08268">paper</a>] [<a href="https://github.com/mayuelala/FollowYourClick">code</a>] [<a href="https://follow-your-click.github.io/">project page</a>] 
	</p>
	</td>
	</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2024-Canvas.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation</b></p>
		<p>Qihua Chen*, <b>Yue Ma*</b>, Hongfa Wang*, Junkun Yuan*, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen, Wei Liu</p>
		<em>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025</em>
		<p> [<a href="hhttps://arxiv.org/abs/2409.01055">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://github.com/mayuelala">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>

		
	<!--########################-->
	<tr>
		<td width="306">
		<img src="./indexpics/2024-Fypv2.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation for Stable Pose Control</b></p>
		<p>Jingyun Xue, Hongfa Wang, Qi Tian, <b>Yue Ma</b>, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao Zhang, Heung-Yeung Shum, Wei Liu, Mengyang Liu, Wenhan Luo</p>
		<em>International Conference on Learning Representations(<b>ICLR</b>) 2025</em>
		<p> [<a href="hhttps://arxiv.org/abs/2406.03035">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://github.com/mayuelala">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<!--########################-->
	<tr></tr>
		<td width="306">
		<img src="./indexpics/2024-followyouremoji.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</b></p>
		<p><b>Yue Ma</b>, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, Qifeng Chen</p>
		<em>The ACM Special Interest Group for Computer Graphics and Interactive Techniques(<b>Siggraph Asia</b>) 2024</em>
		<p> [<a href="https://arxiv.org/abs/2406.01900">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://follow-your-emoji.github.io/">project page</a>] 
		</p>
		</td>
	</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2023-MagicStick.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Handle: Controllable Video Editing via Control Handle Transformations</b></p>
		<p><b>Yue Ma</b>, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, Qifeng Chen</p>
		<em>IEEE /CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2025</em>
		<p> [<a href="https://arxiv.org/abs/2312.03047">paper</a>] [<a href="https://github.com/mayuelala/MagicStick">code</a>] [<a href="https://magic-stick-edit.github.io/">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<!--########################-->
	<tr>
		<td width="306">
		<img src="./indexpics/2023-iccv-followyourpose.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>ğŸ•ºğŸ•ºğŸ•º Follow-Your-Pose ğŸ’ƒğŸ’ƒğŸ’ƒ: Pose-Guided Text-to-Video Generation using Pose-Free Videos</b></p>
		<p><b>Yue Ma</b>, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Ying Shan, Xiu Li, Qifeng Chen</p>
		<em>The 38th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024</em>
		<p><a href="https://www.paperdigest.org/2024/09/most-influential-aaai-papers-2024-09/" style="color: red; display: inline;"><b>PaperDigest Most Influential Papers of AAAI 24</b></a></p>
		<p> [<a href="https://arxiv.org/abs/2304.01186">paper</a>] [<a href="https://github.com/mayuelala/FollowYourPose">code</a>] [<a href="https://follow-your-pose.github.io/">project page</a>] 
		<a target="_blank" href ="https://github.com/mayuelala/FollowYourPose"><img alt="GitHub stars" align="right" src="https://img.shields.io/github/stars/mayuelala/FollowYourPose?style=social"></a><a></a></p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<!--########################-->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2024-AAAI-MBEV.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>M-BEV: Masked BEV Perception for Robust Autonomous Driving</b></p>
		<p>Siran Chen, <b>Yue Ma</b>, Yu Qiao, Yali Wang</p>
		<em>The 38th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024</em>
		<p> [<a href="https://arxiv.org/abs/2304.01186">paper</a>] [<a href="https://github.com/mayuelala/FollowYourPose">code</a>] [<a href="https://follow-your-pose.github.io/">project page</a>] 
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr> -->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2023-icassp-audio.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>SemanticAC: Semantics-Assisted Framework for Audio Classification</b></p>
		<p>Yicheng Xiao*, <b>Yue Ma*</b>, Shuyan Li, Hantao Zhou, Ran Liao, Xiu Li (* equal contribution)</p>
		<em>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>), 2023. </em>
		<i></i>
		<p> [<a href="https://arxiv.org/abs/2302.05940">paper</a>] [<a href="https://github.com/mayuelala">code</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr> -->
	<tr>
		<td width="306">
		<img src="./indexpics/2022-mm-graph.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Visual Knowledge Graph for Human Action Reasoning in Videos</b></p>
		<p><b>Yue Ma</b>, Yali Wang, Yue Wu, Ziyu Lyu, Siran Chen, Xiu Li, Yu Qiao</p>
		<em>The 30th ACM International Conference on Multimedia. (<b>ACM MM</b>), 2022. </em>
		<i><p style="color: red; display: inline;">(<b>Oral Presentation</b>)</p></i>
		<p> [<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548257">paper</a>] [<a href="https://github.com/mayuelala/AKU">code</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    	<tr>&nbsp</tr>
    	<tr>&nbsp</tr>
		
	<!-- <td><b>/*Journal*/</b>
		<p></p>
		</td>
		
		<tr>
			<td width="306">
			<img src="./indexpics/2023-TMM-mmlab.png" width="285px" style="box-shadow: 4px 4px 8px #888">
			</td>				
			<td>
			<p><b>Attentive Snippet Prompting for Video Retrieval</b></p>
			<p>Siran Chen, Qinglin Xu, <b>Yue Ma</b>, Yu Qiao, Yali Wang</p>
			<em>IEEE Transactions on Multimedia (<b>TMM</b>), 2024. </em>
			<i></i>
			<p> [<a href="https://ieeexplore.ieee.org/abstract/document/10268993/">paper</a>] [<a href="https://ieeexplore.ieee.org/abstract/document/10268993/">code</a>] </p>
			</td>
		</tr> -->
		<tr>&nbsp</tr>
			<tr>&nbsp</tr>
			<tr>&nbsp</tr>
			<tr>&nbsp</tr>
			<tr>&nbsp</tr>
	

</tbody></table>
<!--
</div>
-->




<h2>Honors &amp; Awards</h2>
<table style="border-spacing:2px">
	<tbody>
	<tr><td> [06/2024] Outstanding graduates student of Beijing.</td></tr>
	<tr><td> [08/2023] First-Class Scholarship of <a herf="https://www.tsinghua.edu.cn/">Tsinghua University</a>.</td></tr>
	<tr><td> [12/2022] First-Class Scholarship of <a herf="https://www.tsinghua.edu.cn/">SIGS</a>, Tsinghua University.</td></tr>
	<tr><td> [03/2022] <a href="https://www.withzz.com/project/detail/99">Tencent Rhino-Bird Research Elite Program</a>, only 72 students in the world admitted to this program.</td></tr>
	<tr><td> [09/2020] Scholarship for Academic Excellence of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>.
	<tr><td> [06/2019] Excellent Scientific Student of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>.
	<tr><td> [09/2019] Scholarship for Academic Excellence of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>
	<tr><td> [09/2018] Excellent Academic Progress Student of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>.
	<tr><td> [06/2018] Scholarship for Academic Excellence of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>.

	</tbody>
</table>


<h2>Teaching</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2025 Fall, 2024 Fall</td><td>Economics 101</td><td>(PKU, 02831110)</td>
		</tr>
	</tbody>
</table>




<div id="footer">
	<div id="footer-text"></div>
</div>
	<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=777777&w=487&t=tt&d=xeJu_Kwek6AfO5eDCKFQ1iDWjzFQPLT_dNcYY3WLmrY&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=5b1717'></script>
	<p><center> &copy;Noah Loake </center></p>


</div>
</body></html>
























